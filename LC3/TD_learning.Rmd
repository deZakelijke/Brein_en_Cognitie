---
title: "TD_learning"
output: pdf_document
---

# Het model

Het Temdoral Difference Model is een algoritme dat gebruikt wordt in Reinforcement Learning. Het model voorspelt een leercurve bij conditionering. Het model kan in tegenstelling tot het Rescorla-Wagner model ook tijd meenemen in het leerproces. Het model updatet elke tijdstap de verwachte beloning voor de states in de wereld. De formule voor de verwachtingswaarde is:
$$V(s_t) = V(s_t) + \alpha[r_{t+1} + \gamma V(s_{t+1}) - V(s_t)]$$
$V(s_t)$ is de verwachtingswaarde ($V$) van state $s$ op tijdstip $t$. De $\alpha$ is de learningrate (een waarde tussen 0 en 1). $r_{t}$ is de beloning die je krijgt op tijdstip $t$. $\gamma$ is de discount factor, dit kan gezien worden als het tijdsverschil tussen de stimulus en de beloning. Des te meer tijdstappen er tussen de verwachte beloning en de daadwerkelijke beloning zit, des te minder belangrijk deze state wordt.
Wij hebben in deze wereld 7 states in een één dimensionale ruimte. Onze agent begint in de middelste state, en zijn beloning bevind zich aan de rechterkant van de wereld. Elke stap loopt de agent willekeurig naar links of rechts en updatet hij de associatiesterkte van de state waar hij vandaan komt. Dit doet hij totdat hij bij één van de uiteindes van de wereld uit komt. Elke state heeft een a priori verwachtingswaarde. Die zullen van links naar rechts oplopen, omdat de enige beloning zich aan de rechterkant van de wereld bevindt. De verwachtingswaarde zal oplopen als:
$$0 \rightarrow 1/6 \rightarrow 2/6 \rightarrow 3/6 \rightarrow 4/6 \rightarrow 5/6 \rightarrow 6/6$$
Waar $0$ en $6/6$ de accepterende states zijn.

Wanneer je dus bijvoorbeeld vanuit de een-na-laatste state naar de state met de reward beweegt, wordt de associatiesterkte van die state als volgens geupdatet met een $\alpha$ van 0.1 en 0.6:
$$0+0.1*(1+1*0-0) = 0.1\\
0+0.6*(1+1*0-0) = 0.6$$
In plaats van de gevraagde functie TD_ZERO, hebben wij een functie geschreven die het model algemener oplost. Onze functie TD_learn neemt naast de attributen alpha, gamma, en episodes die TD_ZERO gebruikt, ook de state waarin de agent zijn random-walk begint en een vector die de reward van elke state in een wereld van lengte N specificeert.
```{r}
TD_learn <- function(begin_state, reward_mat, alpha, gamma, episodes)
{
  values <- integer(length(reward_mat))
  for(i in 1:episodes)
  {
    current_state = begin_state
    while(current_state != 1 && current_state != length(reward_mat))
    {
      if(rbinom(1, 1, 0.5) == 0)
      {
        next_state = current_state - 1
      }
      else
      {
        next_state  = current_state + 1
      }
      delta = reward_mat[next_state] + gamma*values[next_state] - values[current_state]
      values[current_state] = values[current_state] + alpha*delta
      current_state = next_state
    }
  }
  return(values)
}
begin_state = 4
reward_mat = c(0, 0, 0, 0, 0, 0, 1)
alpha = 0.1
gamma = 1
episodes = 100

TD_learn(begin_state, reward_mat, alpha, gamma, episodes)
```

Hieronder wordt het effect van verschillende waardes van gamma met elkaar vergeleken. Rood heeft de hoogste waarde van gamma, waardoor de vorige waarde het meeste meegenomen wordt, groen heeft een lagere waarde voor gamma, en blauw de laagste. Zoals te zien is, zorgt een hogere waarde van gamma ervoor dat states met een grotere afstand tot de reward een hogere waarde van $V$ krijgen.
```{r}
begin_state = 4
reward_mat = c(0, 0, 0, 0, 0, 0, 1)
alpha = 0.1
episodes = 100

gamma = 0.1
data2 = TD_learn(begin_state, reward_mat, alpha, gamma, episodes)
plot(seq(1, length(reward_mat)), data2, xlab = "State", ylab = "V", ylim =c(0, 1))
lines(seq(1, length(reward_mat)), data2, col="blue",lwd=2)

gamma = 0.5
data1 = TD_learn(begin_state, reward_mat, alpha, gamma, episodes)
lines(seq(1, length(reward_mat)), data1, col="green",lwd=2)
points(seq(1, length(reward_mat)), data1, col="black")

gamma = 0.9
data3 = TD_learn(begin_state, reward_mat, alpha, gamma, episodes)
lines(seq(1, length(reward_mat)), data3, col="red",lwd=2)
points(seq(1, length(reward_mat)), data3, col="black")
```

```{r}
MSE <-function(v, expected_v)
{
  v_diff = v-expected_v
  return(sum(v_diff*v_diff))
}

begin_state = 4
reward_mat = c(0, 0, 0, 0, 0, 0, 1)
alpha = 0.1
gamma = 1
episodes = 100

data = TD_learn(begin_state, reward_mat, alpha, gamma, episodes)
expected_data = c(0, 1/6, 2/6, 3/6, 4/6, 5/6, 0)
MSE(data, expected_data)
```
Hieronder hebben wij de mean squared error van 2 verschillende learning rates. De groene lijn heeft een hogere learning rate dan de rode lijn. Bij een hogere learning rate $\alpha$ wordt de error sneller laag, maar de lagere learning rate berijkt na meer episodes een lagere MSE dan de groene lijn. Dit komt doordat het groene model last heeft van overfitting.
```{r}
TD_MSE <- function(begin_state, reward_mat, alpha, gamma, episodes, expected_v)
{
  MSE = numeric(episodes)
  for(i in 1:episodes)
  {
    v = TD_learn(begin_state, reward_mat, alpha, gamma, i)
    MSE[i] = MSE(v, expected_v)
  }
  return(MSE)
}

begin_state = 4
reward_mat = c(0, 0, 0, 0, 0, 0, 1)
alpha = 0.1
gamma = 1
episodes = 100
expected_data = c(0, 1/6, 2/6, 3/6, 4/6, 5/6, 0)

data1 = TD_MSE(begin_state, reward_mat, alpha, gamma, episodes, expected_data)
plot(seq(1, episodes), data1, xlab = "Number of episodes", ylab = "MSE")
lines(seq(1, episodes), data1, col="red",lwd=2)

alpha = 0.7
data2 = TD_MSE(begin_state, reward_mat, alpha, gamma, episodes, expected_data)
points(seq(1, episodes), data2, xlab = "Number of episodes", ylab = "MSE")
lines(seq(1, episodes), data2, col="green",lwd=2)
```

# Temporal prediction error
Ook temporal difference learning draait op de voorspelllingsfouten tussen het model en de daadwerkelijke waarden. In de formule 
$\delta = (Reward + \gamma * V_S[t+1]- V_S[t])$ is de voorspellingsfout weergegeven als $\delta$.
In het volgende voorbeeld hebben we een agent die, als hij in state één komt naar state vijf gaat en daar een reward krijgt.
Zoals in de grafiek hieroder te zien is, is er na één episode alleen een voorspellingsfout bij de vierde state. Grijze bars zijn de voorspellingsfout, de rode lijn de geleerde V waarden. De voorspellinsfout is, doordat het de eerste epsiode is en er nog niks geleerd is, een voorspellingsfout die gelijk is aan de reward in state vijf.
Allen in state vier is de verwachtingswaarde dus verandert. In state één en vijf is die nul.
Als de agent in een latere situatie weer bij state één arriveert en naar state vijf beweegt zal de voorsplingsfout in state vier kleiner zijn.


```{r}
TD_error <- function(begin_state, reward_mat, alpha, gamma, episodes)
{
  values <- integer(length(reward_mat))
  for(i in 1:episodes)
  {
    current_state = begin_state
    prediction_error = c()
    while(current_state != 5)
    {
      next_state = current_state + 1
      delta = reward_mat[next_state] + gamma*values[next_state] - values[current_state]
      prediction_error <- c(prediction_error, delta)
      values[current_state] = values[current_state] + alpha*delta
      current_state = next_state
    }
  }
  return(list(values, prediction_error))
}

begin_state = 1
reward_mat = c(0, 0, 0, 0, 1)
alpha = 0.1
gamma = 1
episodes = 1

data_error = TD_error(begin_state, reward_mat, alpha, gamma, episodes)

barplot(data_error[[2]], ylim=c(0,1))
lines(seq(1, length(reward_mat)), data_error[[1]], col="red",lwd=2)

```

Als er meer episodes zijn verandert de voorspelingsfout. In de grafiek hieronder, na ten episodes, is te zien dat de voorspellingsfout kleiner is geworden en de geleerde verwachtingswaarde hoger. Grijze bars zijn de voorspellingsfout, de rode lijn de geleerde V waarden. 
In de grafiek daar onder is er voor honderd episodes geleerd. Daar is duidelijk te zien dat de agent nagenoeg geen voorspellingsfout meer heeft en geleerd heeft dat, als hij eenmal naar de vijfde state is gaan lopen, dat hij altijd een reward verwacht. In de laatste state is de verwachtingswaarde weer nul, doordat de agent in die state geen toekomsite reward verwacht.

```{r}
begin_state = 1
reward_mat = c(0, 0, 0, 0, 1)
alpha = 0.1
gamma = 1
episodes = 10

data_error = TD_error(begin_state, reward_mat, alpha, gamma, episodes)

barplot(data_error[[2]], ylim=c(0,1))
lines(seq(1, length(reward_mat)), data_error[[1]], col="red",lwd=2)


episodes = 100

data_error = TD_error(begin_state, reward_mat, alpha, gamma, episodes)

barplot(data_error[[2]], ylim=c(0,1))
lines(seq(1, length(reward_mat)), data_error[[1]], col="red",lwd=2)

```

# Eligibility traces
Om het model te kunnen verbeteren kan een eligibility trace toegevoegd worden. Dit houdt in dat ook states die langer dan één tijdstap geleden bezocht zijn een update kunnen krijgen. Het bijwerken van $\delta$ wordt nu gedaan door de volgende formule: $\Delta V_{(s)} = \alpha * \delta_t * e_{(s)}$ waarbij $\delta_t$ gelijk is aan eerdere formules. De waarde van $e_{(s)}$ wordt bepaald door:
$ e_t (s) = 1$ iff $ s=s_t$
$ e_t (s)  = \gamma *\lambda * e_{t-1}(s)$ else
In de onderstaande grafiek is het leerproces herhaald maar dan met de toevoeging van de eligibility trace. Er zijn drie leercurves te zien. De rode leercurve heeft $\lambda=0.8$, de groene leercurve heeft $\lambda=0.5$ en de blauwe lercurve heeft $\lambda=0.1$. Na een aantal herhalingen van het experiment zijn er geen significante verchillen te zien tussen de drie leercurves. Doordat de wereld in dit model erg klein is. Dit heeft als gevolg dat een state waar de agent relatief vor de werled lang geleden was, absoluut gezien maximaal vijf states geleden is. Hierdoor werkt de eligibility trace net als een geode toevoegin voor het model.


```{r}
TD_lambda <- function(begin_state, reward_mat, alpha, gamma, episodes, lambda)
{
  values <- integer(length(reward_mat))
  
  
  for(i in 1:episodes)
  {
    ET <- integer(length(reward_mat))
    ET_prev <- integer(length(reward_mat))
    
    current_state = begin_state
    while(current_state != 1 && current_state != length(reward_mat))
    {
      if(rbinom(1, 1, 0.5) == 0)
      {
        next_state = current_state - 1
      }
      else
      {
        next_state  = current_state + 1
      }
     
      
      for(i in 1:length(ET_prev)){
        temp = ET[i]
        ET[i] = gamma*lambda*ET_prev[i]
        ET_prev[i] = temp
      }
      ET[current_state] = 1
      
      for(i in 2:(length(reward_mat)-1))
      {
        if(ET[i] != 0){
          delta = reward_mat[next_state] + gamma*values[next_state] - values[i]
          values[i] = values[i] + alpha*delta*ET[i]
        }
      }
      
      
      current_state = next_state
    }
  }
  return(values)
}

begin_state = 4
reward_mat = c(0, 0, 0, 0, 0, 0, 1)
alpha = 0.1
gamma = 1.0
episodes = 1000

lambda = 0.8
data_lambda_1 = TD_lambda(begin_state, reward_mat, alpha, gamma, episodes, lambda)

lambda = 0.5
data_lambda_2 = TD_lambda(begin_state, reward_mat, alpha, gamma, episodes, lambda)

lambda = 0.1
data_lambda_3 = TD_lambda(begin_state, reward_mat, alpha, gamma, episodes, lambda)



plot(seq(1, length(reward_mat)), data_lambda_1, xlab = "State", ylab = "Value", ylim=c(0,1))
lines(seq(1, length(reward_mat)), data_lambda_1, col="red",lwd=2)

points(seq(1, length(reward_mat)), data_lambda_2, xlab = "State", ylab = "Value")
lines(seq(1, length(reward_mat)), data_lambda_2, col="green",lwd=2)

points(seq(1, length(reward_mat)), data_lambda_3, xlab = "State", ylab = "Value")
lines(seq(1, length(reward_mat)), data_lambda_3, col="blue",lwd=2)
```