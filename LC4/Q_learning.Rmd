# Q-learning

Micha de Groot en Roan de Jong

```{r}
Q_step <- function(reward, state, Q_matrix, alpha){
  delta = reward - Q_matrix[state]
  Q_matrix[state] = Q_matrix[state]+delta*alpha
  return(Q_matrix)
}

Q_pick_random <- function(Q_matrix, alpha, means, std){
  states = seq(1, length(Q_matrix))
  state = sample(states, 1)
  reward = rnorm(1, means[state], std)
  Q_matrix = Q_step(reward, state, Q_matrix, alpha)
  return(list(Q_matrix,reward))
}

Q_learn <- function(Q_matrix, alpha, epsilon, means, std, trials){
  reward_sum = 0
  for (i in 1:trials){
    rand_val = runif(1)
    if (epsilon < rand_val){
      state = which.max(Q_matrix)
      reward = rnorm(1, means[state], std)
      reward_sum = reward_sum + reward
      Q_matrix = Q_step(reward, state, Q_matrix, alpha) 
    }else{
      result = Q_pick_random(Q_matrix, alpha, means, std)
      Q_matrix = result[[1]]
      reward_sum = reward_sum + result[[2]]
    }
  }
  return(list(Q_matrix, reward_sum))
}

```

```{r}


alpha = c(0.1, 0.5, 0.8)
epsilon = c(0.05, 0.2, 0.6)
means = c(20, 30 ,50, 70)
Q_matrix = integer(4)
std = 4
trials = 200

for (i in 1:length(epsilon)){
  result = Q_learn(Q_matrix, alpha[i], epsilon[i], means, std, trials)
  print(result[[1]])
  print(result[[2]])
}

```

```{r}
alpha = 0.3
epsilon = c(0.05, 0.2, 0.6)
means = c(20, 30 ,50, 70)
Q_matrix = integer(4)
std = 4
trials = 500

for (i in 1:length(epsilon)){
  result = Q_learn(Q_matrix, alpha, epsilon[i], means, std, trials)
  print(result[[1]])
  print(result[[2]])
}

```

```{r}
Q_learn_decay <- function(Q_matrix, alpha, epsilon, means, std, trials, decay_rate){
  reward_sum = 0
  for (i in 1:trials){
    rand_val = runif(1)
    if (epsilon < rand_val){
      state = which.max(Q_matrix)
      reward = rnorm(1, means[state], std)
      reward_sum = reward_sum + reward
      Q_matrix = Q_step(reward, state, Q_matrix, alpha) 
    }else{
      result = Q_pick_random(Q_matrix, alpha, means, std)
      Q_matrix = result[[1]]
      reward_sum = reward_sum + result[[2]]
    }
   
    epsilon = epsilon*decay_rate 
  }
  return(list(Q_matrix, reward_sum))
}
```

```{r}
alpha = 0.3
epsilon = c(0.05, 0.2, 0.6)
means = c(20, 30 ,50, 70)
Q_matrix = integer(4)
std = 4
trials = 500
decay_rate = 0.99

for (i in 1:length(epsilon)){
  result = Q_learn_decay(Q_matrix, alpha, epsilon[i], means, std, trials, decay_rate)
  print(result[[1]])
  print(result[[2]])
}
```
```{r}
soft_max <- function(Q_matrix, theta){
  probabilities = integer(length(Q_matrix))
  denom = 0
  for (i in 1:length(Q_matrix)){
    denom = denom + exp(Q_matrix[i]*theta)
  }
  for (i in 1:length(Q_matrix)){
    probabilities[i] = exp(Q_matrix[i]*theta) / denom
  }
  return(probabilities)
}

Q_learn_smax <- function(Q_matrix, alpha, theta, means, std, trials){
  reward_sum = 0
  probabilities = rep(1, length(Q_matrix))
  probabilities = probabilities/length(probabilities)
 
  for (i in 1:trials){
    state = sample(1:length(Q_matrix), 1, prob = probabilities)
    reward = rnorm(1, means[state], std)
    reward_sum = reward_sum + reward
    Q_matrix = Q_step(reward, state, Q_matrix, alpha)
    
    probabilities = soft_max(Q_matrix, theta)
  }
  return(list(Q_matrix, reward_sum))
}


alpha = 0.3
theta = seq(from=0.05, to=0.8, by=0.02)
means = c(20, 30 ,50, 70)
Q_matrix = integer(4)
std = 4
trials = 1000
results = integer(length(theta))

for (j in 1:10){
  for (i in 1:length(theta)){
    results[i] =  (Q_learn_smax(Q_matrix, alpha, theta[i], means, std, trials))[[2]]
  }
  plot(theta, results, ylim=c(0,100000))  
}

# theta waarde tussen 0.05 en 0.15 is optimaal. Geeft de meest consistente resultaten


```

